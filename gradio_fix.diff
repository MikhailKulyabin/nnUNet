diff --git a/nnunetv2/inference/data_iterators.py b/nnunetv2/inference/data_iterators.py
index 2486bf6..fcdbf0b 100644
--- a/nnunetv2/inference/data_iterators.py
+++ b/nnunetv2/inference/data_iterators.py
@@ -57,8 +57,46 @@ def preprocess_fromfiles_save_to_queue(list_of_lists: List[List[str]],
         abort_event.set()
         raise e
 
-
 def preprocessing_iterator_fromfiles(list_of_lists: List[List[str]],
+                                        list_of_segs_from_prev_stage_files: Union[None, List[str]],
+                                        output_filenames_truncated: Union[None, List[str]],
+                                        plans_manager: PlansManager,
+                                        dataset_json: dict,
+                                        configuration_manager: ConfigurationManager,
+                                        num_processes: int,
+                                        pin_memory: bool = False,
+                                        verbose: bool = False):
+    # single process iterator version
+    print('SINGE ITERATOR')
+    num_processes = 1
+    assert num_processes >= 1
+
+    for i in range(len(list_of_lists)):
+        data, seg, data_properties = configuration_manager.preprocessor_class(verbose=verbose).run_case(
+            list_of_lists[i],
+            list_of_segs_from_prev_stage_files[i] if list_of_segs_from_prev_stage_files is not None else None,
+            plans_manager,
+            configuration_manager,
+            dataset_json)
+        if list_of_segs_from_prev_stage_files is not None and list_of_segs_from_prev_stage_files[i] is not None:
+            seg_onehot = convert_labelmap_to_one_hot(seg[0], plans_manager.get_label_manager(dataset_json).foreground_labels,
+                                                     data.dtype)
+            data = np.vstack((data, seg_onehot))
+
+        data = torch.from_numpy(data).to(dtype=torch.float32, memory_format=torch.contiguous_format)
+
+        item = {'data': data, 'data_properties': data_properties,
+                'ofile': output_filenames_truncated[i] if output_filenames_truncated is not None else None}
+        if pin_memory:
+            [i.pin_memory() for i in item.values() if isinstance(i, torch.Tensor)]
+        yield item
+        
+
+
+
+
+
+def multi_preprocessing_iterator_fromfiles(list_of_lists: List[List[str]],
                                      list_of_segs_from_prev_stage_files: Union[None, List[str]],
                                      output_filenames_truncated: Union[None, List[str]],
                                      plans_manager: PlansManager,
diff --git a/nnunetv2/inference/predict_from_raw_data.py b/nnunetv2/inference/predict_from_raw_data.py
index 1f5ede6..24127c7 100644
--- a/nnunetv2/inference/predict_from_raw_data.py
+++ b/nnunetv2/inference/predict_from_raw_data.py
@@ -345,78 +345,105 @@ class nnUNetPredictor(object):
         each element returned by data_iterator must be a dict with 'data', 'ofile' and 'data_properties' keys!
         If 'ofile' is None, the result will be returned instead of written to a file
         """
-        with multiprocessing.get_context("spawn").Pool(num_processes_segmentation_export) as export_pool:
-            worker_list = [i for i in export_pool._pool]
-            r = []
-            for preprocessed in data_iterator:
-                data = preprocessed['data']
-                if isinstance(data, str):
-                    delfile = data
-                    data = torch.from_numpy(np.load(data))
-                    os.remove(delfile)
-
-                ofile = preprocessed['ofile']
-                if ofile is not None:
-                    print(f'\nPredicting {os.path.basename(ofile)}:')
-                else:
-                    print(f'\nPredicting image of shape {data.shape}:')
-
-                print(f'perform_everything_on_device: {self.perform_everything_on_device}')
-
-                properties = preprocessed['data_properties']
-
-                # let's not get into a runaway situation where the GPU predicts so fast that the disk has to b swamped with
-                # npy files
-                proceed = not check_workers_alive_and_busy(export_pool, worker_list, r, allowed_num_queued=2)
-                while not proceed:
-                    sleep(0.1)
-                    proceed = not check_workers_alive_and_busy(export_pool, worker_list, r, allowed_num_queued=2)
-
-                prediction = self.predict_logits_from_preprocessed_data(data).cpu()
-
-                if ofile is not None:
-                    # this needs to go into background processes
-                    # export_prediction_from_logits(prediction, properties, self.configuration_manager, self.plans_manager,
-                    #                               self.dataset_json, ofile, save_probabilities)
-                    print('sending off prediction to background worker for resampling and export')
-                    r.append(
-                        export_pool.starmap_async(
-                            export_prediction_from_logits,
-                            ((prediction, properties, self.configuration_manager, self.plans_manager,
-                              self.dataset_json, ofile, save_probabilities),)
-                        )
-                    )
-                else:
-                    # convert_predicted_logits_to_segmentation_with_correct_shape(
-                    #             prediction, self.plans_manager,
-                    #              self.configuration_manager, self.label_manager,
-                    #              properties,
-                    #              save_probabilities)
-
-                    print('sending off prediction to background worker for resampling')
-                    r.append(
-                        export_pool.starmap_async(
-                            convert_predicted_logits_to_segmentation_with_correct_shape, (
-                                (prediction, self.plans_manager,
-                                 self.configuration_manager, self.label_manager,
-                                 properties,
-                                 save_probabilities),)
-                        )
-                    )
-                if ofile is not None:
-                    print(f'done with {os.path.basename(ofile)}')
-                else:
-                    print(f'\nDone with image of shape {data.shape}:')
-            ret = [i.get()[0] for i in r]
-
-        if isinstance(data_iterator, MultiThreadedAugmenter):
-            data_iterator._finish()
-
-        # clear lru cache
-        compute_gaussian.cache_clear()
-        # clear device cache
-        empty_cache(self.device)
-        return ret
+        # single process
+        print('PREDICTING SINGLE PROCESS')
+        results = []
+        for preprocessed in data_iterator:
+            print(preprocessed)
+            data = preprocessed['data']
+            if isinstance(data, str):
+                delfile = data
+                data = torch.from_numpy(np.load(data))
+                os.remove(delfile)
+
+            ofile = preprocessed['ofile']
+            properties = preprocessed['data_properties']
+
+            prediction = self.predict_logits_from_preprocessed_data(data).cpu()
+            hmap = convert_predicted_logits_to_segmentation_with_correct_shape(prediction, self.plans_manager,
+                                                                        self.configuration_manager, self.label_manager,
+                                                                        properties, save_probabilities)
+
+            print(f'\nDone with image of shape {data.shape}:')
+            results.append(hmap)
+        return results
+        
+        
+
+
+
+        # with multiprocessing.get_context("spawn").Pool(num_processes_segmentation_export) as export_pool:
+        #     worker_list = [i for i in export_pool._pool]
+        #     r = []
+        #     for preprocessed in data_iterator:
+        #         data = preprocessed['data']
+        #         if isinstance(data, str):
+        #             delfile = data
+        #             data = torch.from_numpy(np.load(data))
+        #             os.remove(delfile)
+
+        #         ofile = preprocessed['ofile']
+        #         if ofile is not None:
+        #             print(f'\nPredicting {os.path.basename(ofile)}:')
+        #         else:
+        #             print(f'\nPredicting image of shape {data.shape}:')
+
+        #         print(f'perform_everything_on_device: {self.perform_everything_on_device}')
+
+        #         properties = preprocessed['data_properties']
+
+        #         # let's not get into a runaway situation where the GPU predicts so fast that the disk has to b swamped with
+        #         # npy files
+        #         proceed = not check_workers_alive_and_busy(export_pool, worker_list, r, allowed_num_queued=2)
+        #         while not proceed:
+        #             sleep(0.1)
+        #             proceed = not check_workers_alive_and_busy(export_pool, worker_list, r, allowed_num_queued=2)
+
+        #         prediction = self.predict_logits_from_preprocessed_data(data).cpu()
+
+        #         if ofile is not None:
+        #             # this needs to go into background processes
+        #             # export_prediction_from_logits(prediction, properties, self.configuration_manager, self.plans_manager,
+        #             #                               self.dataset_json, ofile, save_probabilities)
+        #             print('sending off prediction to background worker for resampling and export')
+        #             r.append(
+        #                 export_pool.starmap_async(
+        #                     export_prediction_from_logits,
+        #                     ((prediction, properties, self.configuration_manager, self.plans_manager,
+        #                       self.dataset_json, ofile, save_probabilities),)
+        #                 )
+        #             )
+        #         else:
+        #             # convert_predicted_logits_to_segmentation_with_correct_shape(
+        #             #             prediction, self.plans_manager,
+        #             #              self.configuration_manager, self.label_manager,
+        #             #              properties,
+        #             #              save_probabilities)
+
+        #             print('sending off prediction to background worker for resampling')
+        #             r.append(
+        #                 export_pool.starmap_async(
+        #                     convert_predicted_logits_to_segmentation_with_correct_shape, (
+        #                         (prediction, self.plans_manager,
+        #                          self.configuration_manager, self.label_manager,
+        #                          properties,
+        #                          save_probabilities),)
+        #                 )
+        #             )
+        #         if ofile is not None:
+        #             print(f'done with {os.path.basename(ofile)}')
+        #         else:
+        #             print(f'\nDone with image of shape {data.shape}:')
+        #     ret = [i.get()[0] for i in r]
+
+        # if isinstance(data_iterator, MultiThreadedAugmenter):
+        #     data_iterator._finish()
+
+        # # clear lru cache
+        # compute_gaussian.cache_clear()
+        # # clear device cache
+        # empty_cache(self.device)
+        # return ret
 
     def predict_single_npy_array(self, input_image: np.ndarray, image_properties: dict,
                                  segmentation_previous_stage: np.ndarray = None,
diff --git a/pyproject.toml b/pyproject.toml
index 5c45e3c..95be129 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -29,8 +29,9 @@ keywords = [
     'nnU-Net',
     'nnunet'
 ]
+
+
 dependencies = [
-    "torch>=2.1.2",
     "acvl-utils>=0.2,<0.3",  # 0.3 may bring breaking changes. Careful!
     "dynamic-network-architectures>=0.3.1,<0.4",  # 0.3.1 and lower are supported, 0.4 may have breaking changes. Let's be careful here
     "tqdm",
